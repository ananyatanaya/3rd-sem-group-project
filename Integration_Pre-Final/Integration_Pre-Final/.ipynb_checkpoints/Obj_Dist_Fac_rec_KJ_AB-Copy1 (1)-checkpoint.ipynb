{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:  3.8.8\n",
      "OpenCV version:  4.0.1\n",
      "Numpy version:  1.19.2\n",
      "Tensorflow version:  2.3.0\n",
      "3.8.8 (default, Feb 24 2021, 15:54:32) [MSC v.1928 64 bit (AMD64)]\n",
      "   ClassId              SignName\n",
      "0        0  Speed limit (20km/h)\n",
      "1        1  Speed limit (30km/h)\n",
      "2        2  Speed limit (50km/h)\n",
      "3        3  Speed limit (60km/h)\n",
      "4        4  Speed limit (70km/h)\n",
      "Streaming video using device...\n",
      "\n",
      "Loading HAAR classifiers...\n",
      "\n",
      "Loading model...\n",
      "\n",
      "Started training model for Komal\n",
      "Model trained successfully for Komal\n",
      "Started training model for Ananya\n",
      "Model trained successfully for Ananya\n",
      "Started training model for Arunima\n",
      "Model trained successfully for Arunima\n",
      "Started training model for Ibrahim\n",
      "Model trained successfully for Ibrahim\n",
      "Distance(cms):58.92857142857143\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-629a9b7d0edf>:77: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if faces == ():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance(cms):58.92857142857143\n",
      "\n",
      "Distance(cms):64.16666666666667\n",
      "\n",
      "Distance(cms):65.625\n",
      "\n",
      "Distance(cms):77.0\n",
      "\n",
      "Distance(cms):79.10958904109589\n",
      "\n",
      "Distance(cms):86.19402985074628\n",
      "\n",
      "Distance(cms):79.10958904109589\n",
      "\n",
      "Distance(cms):84.92647058823529\n",
      "\n",
      "Distance(cms):84.92647058823529\n",
      "\n",
      "Distance(cms):148.07692307692307\n",
      "\n",
      "Distance(cms):82.5\n",
      "\n",
      "Distance(cms):134.30232558139534\n",
      "\n",
      "Distance(cms):148.07692307692307\n",
      "\n",
      "Distance(cms):169.85294117647058\n",
      "\n",
      "Distance(cms):175.0\n",
      "\n",
      "Distance(cms):175.0\n",
      "\n",
      "Distance(cms):175.0\n",
      "\n",
      "Distance(cms):175.0\n",
      "\n",
      "Distance(cms):175.0\n",
      "\n",
      "Distance(cms):175.0\n",
      "\n",
      "Distance(cms):175.0\n",
      "\n",
      "Distance(cms):175.0\n",
      "\n",
      "Distance(cms):175.0\n",
      "\n",
      "Distance(cms):175.0\n",
      "\n",
      "Elapsed time: 63.51\n",
      "Approximate FPS: 0.38\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import sys\n",
    "import cv2\n",
    "from math import pow, sqrt\n",
    "import imutils\n",
    "from imutils.video import VideoStream\n",
    "from imutils.video import FPS\n",
    "import time\n",
    "import os\n",
    "from os import listdir\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import pandas as pd\n",
    "from os.path import isfile, join\n",
    "from numpy.core.records import array\n",
    "from platform import python_version\n",
    "\n",
    "print(\"Python version: \", python_version())\n",
    "print(\"OpenCV version: \", cv2.__version__)\n",
    "print(\"Numpy version: \", np.version.version)\n",
    "print(\"Tensorflow version: \", tf.__version__)\n",
    "print(sys.version)\n",
    "\n",
    "# Parse the arguments from command line\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('-v', '--video', type = str, default = 'traffic_videos/foggymorning.mp4', help = 'Video file path. If no path is given, video is captured using device.')\n",
    "\n",
    "parser.add_argument('-m', '--model', default = 'SSD_MobileNet.caffemodel', help = \"Path to the pretrained model.\")\n",
    "    \n",
    "parser.add_argument('-p', '--prototxt', default = 'SSD_MobileNet_prototxt.txt', help = 'Prototxt of the model.')\n",
    "\n",
    "parser.add_argument('-l', '--labels', default = 'class_labels.txt', help = 'Labels of the dataset.')\n",
    "\n",
    "parser.add_argument('-y', '--cfg', default = 'yolov3.cfg', help = 'Path_to_yolo_caffemodel')\n",
    "\n",
    "parser.add_argument('-w', '--weights', default = 'yolov3.weights', help = 'Prototxt file for yolo')\n",
    "\n",
    "parser.add_argument('-x', '--excel', default = 'label_names.csv', help = 'CSV file for Traffic_Sign_Detection')\n",
    "\n",
    "parser.add_argument('-c', '--confidence', type = float, default = 0.9, help='Set confidence for detecting objects')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "# Loading trained CNN model to use it later when classifying from 4 groups into one of 43 classes\n",
    "model = load_model('model-23x23.h5')\n",
    "\n",
    "# Loading mean image to use for preprocessing further; Opening file for reading in binary mode\n",
    "with open('mean_image_rgb.pickle', 'rb') as f:\n",
    "    mean = pickle.load(f, encoding='latin1')  # dictionary type\n",
    "\n",
    "labels = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\",\"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\",\"diningtable\",\n",
    "            \"dog\",\"horse\", \"motorbike\",\"person\", \"pottedplant\", \"sheep\",\"sofa\", \"train\", \"tvmonitor\"]\n",
    "COLORS = np.random.uniform(0, 255, size=(len(labels), 3))\n",
    "# Read the csv file for traffic-sign and print first five records\n",
    "tf_labels = pd.read_csv(args.excel)\n",
    "print(tf_labels.head())\n",
    "\n",
    "print(\"Streaming video using device...\\n\")\n",
    "\n",
    "# Load HAAR face classifier\n",
    "face_classifier = cv2.CascadeClassifier('haarcascade_features/haarcascade_frontalface_default.xml')\n",
    "profile_classifier = cv2.CascadeClassifier('haarcascade_features/haarcascade_profileface.xml')\n",
    "eye_classifier = cv2.CascadeClassifier('haarcascade_features/haarcascade_eye.xml')\n",
    "print(\"Loading HAAR classifiers...\\n\")\n",
    "\n",
    "\n",
    "# Function to detect face\n",
    "def face_detector(img, size=0.5):\n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale( gray, 1.3, 5, minSize = (30,30))\n",
    "    # If face not found return blank region\n",
    "    if faces == ():\n",
    "        return [img, [], None]\n",
    "    # Obtain Region of face\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img, (x,y), (x+w,y+h), (0,255,255),2)\n",
    "        roi = img[y:y+h, x:x+w]\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi = cv2.resize(roi, (200, 200))        \n",
    "        profile = profile_classifier.detectMultiScale(img, 1.3,5)\n",
    "        for (px,py,pw,ph) in profile:\n",
    "            cv2.rectangle(img,(px,py),(px+pw,py+ph), (0,255,255),2)         \n",
    "        eyes = eye_classifier.detectMultiScale(img, 1.3,4)\n",
    "        for (ex,ey,ew,eh) in eyes:\n",
    "            cv2.rectangle(img,(ex,ey),(ex+ew,ey+eh), (0,255,255),2) \n",
    "    return [img, roi, faces[0]]   \n",
    "\n",
    "# Capture video from file or through device (webcam)\n",
    "if args.video:\n",
    "    cap = cv2.VideoCapture(args.video)\n",
    "else:\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "#initialize the FPS counter\n",
    "fps = FPS().start()\n",
    "#Load the Caffe model \n",
    "print(\"Loading model...\\n\")\n",
    "net = cv2.dnn.readNetFromCaffe(args.prototxt, args.model)\n",
    "d_net = cv2.dnn.readNetFromDarknet(args.cfg, args.weights)\n",
    "# Getting names of all YOLO v3 layers\n",
    "layers_all = d_net.getLayerNames()\n",
    "# Getting only detection YOLO v3 layers that are 82, 94 and 106\n",
    "layers_names_output = [layers_all[i[0] - 1] for i in d_net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Facial Recognition model training \n",
    "models = {\"Komal\": {\"data_path\": \"face/komal/\",\"files\": [],\"model\": None},\n",
    "          \"Ananya\": {\"data_path\": \"face/ananya/\",\"files\": [],\"model\": None},\n",
    "          \"Arunima\": {\"data_path\": \"face/arunima/\",\"files\": [],\"model\": None},\n",
    "          \"Ibrahim\": {\"data_path\": \"face/ibrahim/\",\"files\": [],\"model\": None}\n",
    "         }\n",
    "for key in models:\n",
    "    print(\"Started training model for \" + key)\n",
    "    models[key][\"files\"] = [f for f in listdir(models[key][\"data_path\"]) if isfile(join(models[key][\"data_path\"], f))]\n",
    "    Training_Data, Labels = [], []\n",
    "\n",
    "    for i, files in enumerate(models[key][\"files\"]):\n",
    "        image_path = models[key][\"data_path\"] + models[key][\"files\"][i]\n",
    "        images = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        Training_Data.append( np.asarray( images, dtype=np.uint8))\n",
    "        Labels.append(i)\n",
    "\n",
    "    # Create a numpy array for both training data and labels\n",
    "    Labels = np.asarray(Labels, dtype=np.int32)\n",
    "\n",
    "    # Initialize facial recognizer\n",
    "    models[key][\"model\"] =  cv2.face.LBPHFaceRecognizer_create()\n",
    "    # NOTE: For OpenCV 3.0 use cv2.face.createLBPHFaceRecognizer()\n",
    "    # Let's train our model\n",
    "    models[key][\"model\"].train(np.asarray(Training_Data), np.asarray(Labels))\n",
    "    print(\"Model trained successfully for \" + key)\n",
    "\n",
    "while True:  \n",
    "    ret, frame = cap.read()\n",
    "    ar = face_detector(frame)\n",
    "    face=ar[1] \n",
    "    pos=ar[2]\n",
    "    time.sleep(0.06)\n",
    "    if not ret:\n",
    "        break    \n",
    "    # grab the frame from the threaded video stream and resize it to have a maximum width of 600 pixels    \n",
    "    frame = imutils.resize(frame, width=600)\n",
    "    # grab the frame dimensions and convert it to a blob\n",
    "    (h, w) = frame.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)),0.007843, (300, 300), 127.5)\n",
    "    # Blob from current frame of traffic sign video\n",
    "    tf_blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416),swapRB=True, crop=False)\n",
    "    # pass the blob through the network and obtain the detections and predictions\n",
    "    net.setInput(blob)\n",
    "    d_net.setInput(tf_blob)\n",
    "    d_net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "    detections = net.forward()\n",
    "    tf_detections = d_net.forward(layers_names_output)\n",
    "       \n",
    "    # loop over the detections\n",
    "    for i in np.arange(0, detections.shape[2]):\n",
    "        # extract the confidence (i.e., probability) associated with the prediction\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        # filter out weak detections by ensuring the `confidence` is greater than the minimum confidence\n",
    "        if confidence > args.confidence:\n",
    "            # extract the index of the class label from the`detections`, then compute the (x, y)coordinates of the bounding box for the object\n",
    "            idx = int(detections[0, 0, i, 1])\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")            \n",
    "            # draw the prediction on the frame\n",
    "            label = \"{}: {:.2f}%\".format(labels[idx],confidence * 100)\n",
    "            cv2.rectangle(frame, (startX, startY), (endX, endY),COLORS[idx], 2)\n",
    "            y = startY - 15 if startY - 15 > 15 else startY + 15\n",
    "            cv2.putText(frame, label, (startY, y),cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLORS[idx], 1)       \n",
    "    \n",
    "    pos_dict = dict()\n",
    "    coordinates = dict()\n",
    "    # Focal length (in cm)\n",
    "    F = 35    \n",
    "    box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "    (startX, startY, endX, endY) = box.astype(\"int\")  \n",
    "    coordinates[i] = (startX, startY, endX, endY)\n",
    "    # Mid point of bounding box\n",
    "    x_mid = round((startX+endX)/2,4)\n",
    "    y_mid = round((startY+endY)/2,4)\n",
    "    height = round(endY-startY,4)\n",
    "\n",
    "    # Distance from camera based on triangle similarity\n",
    "    distance = (165 * F)/height\n",
    "    print(\"Distance(cms):{dist}\\n\".format(dist=distance))  \n",
    "    \n",
    "    # Mid-point of bounding boxes (in cm) based on triangle similarity technique\n",
    "    x_mid_cm = (x_mid * distance) / F\n",
    "    y_mid_cm = (y_mid * distance) / F\n",
    "    pos_dict[i] = (x_mid_cm,y_mid_cm,distance)\n",
    "    \n",
    "    # Distance between every object detected in a frame\n",
    "    close_objects = set()\n",
    "    for i in pos_dict.keys():\n",
    "        for j in pos_dict.keys():\n",
    "            if i < j:\n",
    "                dist = sqrt(pow(pos_dict[i][0]-pos_dict[j][0],2) + pow(pos_dict[i][1]-pos_dict[j][1],2) + pow(pos_dict[i][2]-pos_dict[j][2],2))\n",
    "\n",
    "                # Check if distance less than 1 feet (200 mm approx):\n",
    "                if dist < 200:\n",
    "                    close_objects.add(i)\n",
    "                    close_objects.add(j)\n",
    "    for i in pos_dict.keys():\n",
    "        if i in close_objects:\n",
    "            COLOR = (0,0,255)\n",
    "        else:\n",
    "            COLOR = (0,255,0)     \n",
    "        (startX, startY, endX, endY) = coordinates[i]\n",
    "        cv2.rectangle(frame,(startX,startY), (endX, endY), COLOR, 1)\n",
    "        y = startY - 15 if startY - 15 > 15 else startY + 15        \n",
    "        # Convert mms to feet\n",
    "        cv2.putText(frame, \"Distance: {i} ft\".format(i=round(pos_dict[i][2]/30.48,4)), (y, startY),cv2.FONT_HERSHEY_SIMPLEX, 0.45, COLOR, 1)\n",
    "        cv2.namedWindow('Frame',cv2.WINDOW_NORMAL) \n",
    "\n",
    "    # Lists for detected bounding boxes, confidences and class's number\n",
    "    bounding_boxes = []\n",
    "    confidences = []\n",
    "    class_numbers = []\n",
    "\n",
    "    # Going through all output layers after feed forward pass\n",
    "    for traffic_result in tf_detections:\n",
    "        # Going through all detections from current output layer\n",
    "        for detected_objects in traffic_result:\n",
    "            # Getting 80 classes' probabilities for current detected object\n",
    "            scores = detected_objects[5:]\n",
    "            # Getting index of the class with the maximum value of probability\n",
    "            class_current = np.argmax(scores)\n",
    "            # Getting value of probability for defined class\n",
    "            confidence_current = scores[class_current]\n",
    "            # Minimum probability to eliminate weak detections\n",
    "            probability_minimum = 0.2\n",
    "            # Setting threshold to filtering weak bounding boxes by non-maximum suppression\n",
    "            threshold = 0.5\n",
    "            \n",
    "            # Eliminating weak predictions by minimum probability\n",
    "            if confidence_current > probability_minimum:\n",
    "                # Scaling bounding box coordinates to the initial frame size\n",
    "                box_current = detected_objects[0:4] * np.array([w, h, w, h])\n",
    "\n",
    "                # Getting top left corner coordinates\n",
    "                x_center, y_center, box_width, box_height = box_current\n",
    "                x_min = int(x_center - (box_width / 2))\n",
    "                y_min = int(y_center - (box_height / 2))\n",
    "\n",
    "                # Adding results into prepared lists\n",
    "                bounding_boxes.append([x_min, y_min, int(box_width), int(box_height)])\n",
    "                confidences.append(float(confidence_current))\n",
    "                class_numbers.append(class_current)                \n",
    "\n",
    "    # Implementing non-maximum suppression of given bounding boxes\n",
    "    tf_results = cv2.dnn.NMSBoxes(bounding_boxes, confidences, probability_minimum, threshold)\n",
    "\n",
    "    # Checking if there is any detected object been left\n",
    "    if len(tf_results) > 0:\n",
    "        # Going through indexes of results\n",
    "        for i in tf_results.flatten():\n",
    "            # Bounding box coordinates, its width and height\n",
    "            x_min, y_min = bounding_boxes[i][0], bounding_boxes[i][1]\n",
    "            box_width, box_height = bounding_boxes[i][2], bounding_boxes[i][3]             \n",
    "            # Cut fragment with Traffic Sign\n",
    "            c_ts = frame[y_min:y_min+int(box_height), x_min:x_min+int(box_width), :]            \n",
    "            if c_ts.shape[:1] == (0,) or c_ts.shape[1:2] == (0,):\n",
    "                pass\n",
    "            else:\n",
    "                # Getting preprocessed blob with Traffic Sign of needed shape\n",
    "                blob_ts = cv2.dnn.blobFromImage(c_ts, 1 / 255.0, size=(32, 32), swapRB=True, crop=False)\n",
    "                blob_ts[0] = blob_ts[0, :, :, :] - mean['mean_image_rgb']\n",
    "                blob_ts = blob_ts.transpose(0, 2, 3, 1)\n",
    "                               \n",
    "                prediction = np.argmax(scores)\n",
    "                \n",
    "                # Feeding to the Keras CNN model to get predicted label among 43 classes\n",
    "                scores = model.predict(blob_ts)\n",
    "                # Generating colours for bounding boxes\n",
    "                # randint(low, high=None, size=None, dtype='l')\n",
    "                #tf_colours = np.random.randint(0, 255, size=(len(tf_labels), 3), dtype='uint8')\n",
    "                # Colour for current bounding box\n",
    "                #colour_box_current = tf_colours[class_numbers[i]].tolist()\n",
    "                \n",
    "                # Drawing bounding box on the original current frame\n",
    "                cv2.rectangle(frame, (x_min, y_min),(x_min + box_width, y_min + box_height),(0,9,255), 2)\n",
    "\n",
    "                # Preparing text with label and confidence for current bounding box\n",
    "                text_box_current = '{}: {:.4f}'.format(tf_labels['SignName'][prediction],confidences[i]*100)\n",
    "\n",
    "                # Putting text with label and confidence on the original image\n",
    "                cv2.putText(frame, text_box_current, (x_min, y_min - 5),cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0,0,250), 1)\n",
    "\n",
    "    try:\n",
    "        face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "        foundFace = False\n",
    "        user = None\n",
    "        confidence = 85\n",
    "        for key in models:\n",
    "            if foundFace == True:\n",
    "                break\n",
    "            results = models[key][\"model\"].predict(face)\n",
    "            if results[1] < 500:\n",
    "                confidence = int( 100 * (1 - (results[1])/500) )\n",
    "                if confidence > 85:\n",
    "                    user = key\n",
    "                    foundFace = True        \n",
    "        posX = pos[0] + 5\n",
    "        posY = pos[0] - 5\n",
    "        cv2.putText(frame, \"Face Detected \" + str(confidence) + \"%\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 1)\n",
    "        if foundFace == True:\n",
    "            cv2.putText(frame, user, (posX, posY), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 1)                \n",
    "        else:\n",
    "            cv2.putText(frame, \"Unknown \", (posX, posY), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,153,255), 1)\n",
    "\n",
    "        cv2.namedWindow('Frame',cv2.WINDOW_NORMAL)\n",
    "    # Raise exception in case, no image is found\n",
    "    except Exception as e:\n",
    "        cv2.putText(frame, \"Accuracy 0% (No face detected)\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,155,255), 1)\n",
    "        cv2.namedWindow('Frame',cv2.WINDOW_NORMAL)\n",
    "        pass\n",
    "    \n",
    "    # Show the output frame\n",
    "    cv2.imshow('Frame', frame)\n",
    "    cv2.resizeWindow('Frame',800,600)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "    #update the FPS counter\n",
    "    fps.update()\n",
    "#stop the timer and display FPS count \n",
    "fps.stop()\n",
    "print(\"Elapsed time: {:.2f}\".format(fps.elapsed()))\n",
    "print(\"Approximate FPS: {:.2f}\".format(fps.fps()))\n",
    "# Clean\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
