{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytesseract\n",
      "  Downloading pytesseract-0.3.7.tar.gz (13 kB)\n",
      "Requirement already satisfied: Pillow in c:\\users\\ananya\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pytesseract) (8.1.2)\n",
      "Building wheels for collected packages: pytesseract\n",
      "  Building wheel for pytesseract (setup.py): started\n",
      "  Building wheel for pytesseract (setup.py): finished with status 'done'\n",
      "  Created wheel for pytesseract: filename=pytesseract-0.3.7-py2.py3-none-any.whl size=13945 sha256=5efa717d26b3a6f99ef10adc5c54d500439f1fa2ecbacd467ed4ffff654327d3\n",
      "  Stored in directory: c:\\users\\ananya\\appdata\\local\\pip\\cache\\wheels\\e5\\22\\c8\\c633fb88695e51343e8a57ea85995bd3c4fd02f110c2d4ac91\n",
      "Successfully built pytesseract\n",
      "Installing collected packages: pytesseract\n",
      "Successfully installed pytesseract-0.3.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.0.1; however, version 21.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\ananya\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n",
      "Python version:  3.9.0\n",
      "OpenCV version:  4.5.1\n",
      "Numpy version:  1.19.5\n",
      "Tensorflow version:  2.5.0-rc1\n",
      "Pickle version:  4.0\n",
      "3.9.0 (tags/v3.9.0:9cf6752, Oct  5 2020, 15:34:40) [MSC v.1927 64 bit (AMD64)]\n",
      "   ClassId              SignName\n",
      "0        0  Speed limit (20km/h)\n",
      "1        1  Speed limit (30km/h)\n",
      "2        2  Speed limit (50km/h)\n",
      "3        3  Speed limit (60km/h)\n",
      "4        4  Speed limit (70km/h)\n",
      "Streaming video using device...\n",
      "\n",
      "Loading HAAR classifiers...\n",
      "\n",
      "Loading model...\n",
      "\n",
      "Started training model for Komal\n",
      "Model trained successfully for Komal\n",
      "Started training model for Ananya\n",
      "Model trained successfully for Ananya\n",
      "Started training model for Arunima\n",
      "Model trained successfully for Arunima\n",
      "Started training model for Ibrahim\n",
      "Model trained successfully for Ibrahim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-0550b91e4088>:191: RuntimeWarning: divide by zero encountered in long_scalars\n",
      "  distance = round(((165 * F)/height)/30.48,2)\n",
      "<ipython-input-1-0550b91e4088>:195: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  x_mid_cm = (x_mid * distance) / F\n",
      "<ipython-input-1-0550b91e4088>:196: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  y_mid_cm = (y_mid * distance) / F\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance:inf feet\n",
      "Distance:inf feet\n",
      "Distance:inf feet\n",
      "Distance:inf feet\n",
      "person: 99.90%\n",
      "Distance:inf feet\n",
      "person: 98.98%\n",
      "Distance:inf feet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-0550b91e4088>:76: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if faces == ():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance:inf feet\n",
      "Komal\n",
      "Face Detected 85%\n",
      "Distance:inf feet\n",
      "Komal\n",
      "Face Detected 86%\n",
      "person: 99.85%\n",
      "Distance:inf feet\n",
      "Komal\n",
      "Face Detected 86%\n",
      "person: 99.68%\n",
      "Distance:inf feet\n",
      "Komal\n",
      "Face Detected 86%\n",
      "person: 99.63%\n",
      "Distance:inf feet\n",
      "Komal\n",
      "Face Detected 85%\n",
      "person: 99.81%\n",
      "Distance:inf feet\n",
      "Komal\n",
      "Face Detected 86%\n",
      "person: 99.58%\n",
      "Distance:inf feet\n",
      "Komal\n",
      "Face Detected 85%\n",
      "Distance:inf feet\n",
      "Komal\n",
      "Face Detected 85%\n",
      "person: 99.06%\n",
      "Distance:inf feet\n",
      "Komal\n",
      "Face Detected 85%\n",
      "person: 99.67%\n",
      "Distance:inf feet\n",
      "Komal\n",
      "Face Detected 85%\n",
      "Distance:inf feet\n",
      "Distance:inf feet\n",
      "Distance:inf feet\n",
      "Distance:inf feet\n",
      "Distance:inf feet\n",
      "Distance:inf feet\n",
      "Distance:inf feet\n",
      "Distance:inf feet\n",
      "Distance:inf feet\n",
      "Distance:inf feet\n",
      "Distance:inf feet\n",
      "Distance:inf feet\n",
      "Distance:inf feet\n",
      "Distance:inf feet\n",
      "Distance:inf feet\n",
      "Distance:inf feet\n",
      "Komal\n",
      "Face Detected 85%\n",
      "person: 99.77%\n",
      "Distance:inf feet\n",
      "Komal\n",
      "Face Detected 86%\n",
      "person: 99.68%\n",
      "Distance:inf feet\n",
      "Komal\n",
      "Face Detected 86%\n",
      "Elapsed time: 151.08\n",
      "Approximate FPS: 0.22\n"
     ]
    },
    {
     "ename": "TesseractNotFoundError",
     "evalue": "C:/Program Files (x86)/Tesseract-OCR/tesseract.exe is not installed or it's not in your PATH. See README file for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\ananya\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pytesseract\\pytesseract.py\u001b[0m in \u001b[0;36mrun_tesseract\u001b[1;34m(input_filename, output_filename_base, extension, lang, config, nice, timeout)\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[0mproc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmd_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msubprocess_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ananya\\appdata\\local\\programs\\python\\python39\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[0;32m    946\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[0;32m    948\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ananya\\appdata\\local\\programs\\python\\python39\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1415\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1416\u001b[1;33m                 hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n\u001b[0m\u001b[0;32m   1417\u001b[0m                                          \u001b[1;31m# no special security\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTesseractNotFoundError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-0550b91e4088>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[0mdemo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[0mpytesseract\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpytesseract\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtesseract_cmd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'C:/Program Files (x86)/Tesseract-OCR/tesseract.exe'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m \u001b[0mtext_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpytesseract\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_to_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdemo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'eng'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_reader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[1;31m# Clean\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ananya\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pytesseract\\pytesseract.py\u001b[0m in \u001b[0;36mimage_to_string\u001b[1;34m(image, lang, config, nice, output_type, timeout)\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 409\u001b[1;33m     return {\n\u001b[0m\u001b[0;32m    410\u001b[0m         \u001b[0mOutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBYTES\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mrun_and_get_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m         \u001b[0mOutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDICT\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mrun_and_get_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ananya\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pytesseract\\pytesseract.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    410\u001b[0m         \u001b[0mOutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBYTES\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mrun_and_get_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m         \u001b[0mOutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDICT\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mrun_and_get_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 412\u001b[1;33m         \u001b[0mOutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSTRING\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mrun_and_get_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    413\u001b[0m     }[output_type]()\n\u001b[0;32m    414\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ananya\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pytesseract\\pytesseract.py\u001b[0m in \u001b[0;36mrun_and_get_output\u001b[1;34m(image, extension, lang, config, nice, timeout, return_bytes)\u001b[0m\n\u001b[0;32m    285\u001b[0m         }\n\u001b[0;32m    286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m         \u001b[0mrun_tesseract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'output_filename_base'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mextsep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mextension\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ananya\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pytesseract\\pytesseract.py\u001b[0m in \u001b[0;36mrun_tesseract\u001b[1;34m(input_filename, output_filename_base, extension, lang, config, nice, timeout)\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mENOENT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mTesseractNotFoundError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtimeout_manager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merror_string\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTesseractNotFoundError\u001b[0m: C:/Program Files (x86)/Tesseract-OCR/tesseract.exe is not installed or it's not in your PATH. See README file for more information."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import sys\n",
    "import cv2\n",
    "from math import pow, sqrt\n",
    "import imutils\n",
    "from imutils.video import VideoStream\n",
    "from imutils.video import FPS\n",
    "import time\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from numpy.core.records import array\n",
    "from platform import python_version\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "print(\"Python version: \", python_version())\n",
    "print(\"OpenCV version: \", cv2.__version__)\n",
    "print(\"Numpy version: \", np.version.version)\n",
    "print(\"Tensorflow version: \", tf.__version__)\n",
    "print(\"Pickle version: \", pickle.format_version)\n",
    "print(sys.version)\n",
    "\n",
    "# Parse the arguments from command line\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('-v', '--video', type = str, default = '', help = 'Video file path. If no path is given, video is captured using device.')\n",
    "\n",
    "parser.add_argument('-m', '--model', default = 'SSD_MobileNet.caffemodel', help = \"Path to the pretrained model.\")\n",
    "    \n",
    "parser.add_argument('-p', '--prototxt', default = 'SSD_MobileNet_prototxt.txt', help = 'Prototxt of the model.')\n",
    "\n",
    "parser.add_argument('-l', '--labels', default = 'class_labels.txt', help = 'Labels of the dataset.')\n",
    "\n",
    "parser.add_argument('-y', '--cfg', default = 'yolov3.cfg', help = 'Path_to_yolo_caffemodel')\n",
    "\n",
    "parser.add_argument('-w', '--weights', default = 'yolov3.weights', help = 'Prototxt file for yolo')\n",
    "\n",
    "parser.add_argument('-x', '--excel', default = 'label_names.csv', help = 'CSV file for Traffic_Sign_Detection')\n",
    "\n",
    "parser.add_argument('-c', '--confidence', type = float, default = 0.9, help='Set confidence for detecting objects')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "\n",
    "# Loading mean image to use for preprocessing further; Opening file for reading in binary mode\n",
    "with open('mean_image_rgb.pickle', 'rb') as f:\n",
    "    mean = pickle.load(f, encoding='latin1')  # dictionary type\n",
    "\n",
    "labels = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\",\"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\",\"diningtable\",\n",
    "            \"dog\",\"horse\", \"motorbike\",\"person\", \"pottedplant\", \"sheep\",\"sofa\", \"train\", \"tvmonitor\"]\n",
    "COLORS = np.random.uniform(0, 255, size=(len(labels), 3))\n",
    "# Read the csv file for traffic-sign and print first five records\n",
    "tf_labels = pd.read_csv(args.excel)\n",
    "print(tf_labels.head())\n",
    "\n",
    "print(\"Streaming video using device...\\n\")\n",
    "\n",
    "# Load HAAR face classifier\n",
    "face_classifier = cv2.CascadeClassifier('haarcascade_features/haarcascade_frontalface_default.xml')\n",
    "profile_classifier = cv2.CascadeClassifier('haarcascade_features/haarcascade_profileface.xml')\n",
    "eye_classifier = cv2.CascadeClassifier('haarcascade_features/haarcascade_eye.xml')\n",
    "print(\"Loading HAAR classifiers...\\n\")\n",
    "\n",
    "\n",
    "# Function to detect face\n",
    "def face_detector(img, size=0.5):\n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale( gray, 1.3, 5, minSize = (30,30))\n",
    "    # If face not found return blank region\n",
    "    if faces == ():\n",
    "        return [img, [], None]\n",
    "    # Obtain Region of face\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img, (x,y), (x+w,y+h), (0,255,255),2)\n",
    "        roi = img[y:y+h, x:x+w]\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi = cv2.resize(roi, (200, 200))        \n",
    "        profile = profile_classifier.detectMultiScale(img, 1.3,5)\n",
    "        for (px,py,pw,ph) in profile:\n",
    "            cv2.rectangle(img,(px,py),(px+pw,py+ph), (0,255,255),2)         \n",
    "        eyes = eye_classifier.detectMultiScale(img, 1.3,4)\n",
    "        for (ex,ey,ew,eh) in eyes:\n",
    "            cv2.rectangle(img,(ex,ey),(ex+ew,ey+eh), (0,255,255),2) \n",
    "    return [img, roi, faces[0]]   \n",
    "\n",
    "# Capture video from file or through webcam\n",
    "if args.video:\n",
    "    cap = cv2.VideoCapture(args.video)    \n",
    "else:\n",
    "    cap = cv2.VideoCapture(0)    \n",
    "#initialize the FPS counter\n",
    "fps = FPS().start()\n",
    "#Load the Caffe model \n",
    "print(\"Loading model...\\n\")\n",
    "net = cv2.dnn.readNetFromCaffe(args.prototxt, args.model)\n",
    "d_net = cv2.dnn.readNetFromDarknet(args.cfg, args.weights)\n",
    "\n",
    "# To use with GPU\n",
    "d_net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "d_net.setPreferableTarget(cv2.dnn.DNN_TARGET_OPENCL_FP16)\n",
    "# Getting names of all YOLO v3 layers\n",
    "layers_all = d_net.getLayerNames()\n",
    "# Getting only detection YOLO v3 layers that are 82, 94 and 106\n",
    "layers_names_output = [layers_all[i[0] - 1] for i in d_net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Facial Recognition model training \n",
    "models = {\"Komal\": {\"data_path\": \"face/komal/\",\"files\": [],\"model\": None},\n",
    "          \"Ananya\": {\"data_path\": \"face/ananya/\",\"files\": [],\"model\": None},\n",
    "          \"Arunima\": {\"data_path\": \"face/arunima/\",\"files\": [],\"model\": None},\n",
    "          \"Ibrahim\": {\"data_path\": \"face/ibrahim/\",\"files\": [],\"model\": None}\n",
    "         }\n",
    "for key in models:\n",
    "    print(\"Started training model for \" + key)\n",
    "    models[key][\"files\"] = [f for f in listdir(models[key][\"data_path\"]) if isfile(join(models[key][\"data_path\"], f))]\n",
    "    Training_Data, Labels = [], []\n",
    "\n",
    "    for i, files in enumerate(models[key][\"files\"]):\n",
    "        image_path = models[key][\"data_path\"] + models[key][\"files\"][i]\n",
    "        images = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        Training_Data.append( np.asarray( images, dtype=np.uint8))\n",
    "        Labels.append(i)\n",
    "\n",
    "    # Create a numpy array for both training data and labels\n",
    "    Labels = np.asarray(Labels, dtype=np.int32)\n",
    "\n",
    "    # Initialize facial recognizer\n",
    "    models[key][\"model\"] =  cv2.face.LBPHFaceRecognizer_create()\n",
    "    # NOTE: For OpenCV 3.0 use cv2.face.createLBPHFaceRecognizer()\n",
    "    # Let's train our model\n",
    "    models[key][\"model\"].train(np.asarray(Training_Data), np.asarray(Labels))\n",
    "    print(\"Model trained successfully for \" + key)\n",
    "\n",
    "while True:  \n",
    "    ret, frame = cap.read()\n",
    "    ar = face_detector(frame)\n",
    "    face=ar[1] \n",
    "    pos=ar[2]\n",
    "    time.sleep(0.06)\n",
    "    if not ret:\n",
    "        break   \n",
    "\n",
    "    # grab the frame from the threaded video stream and resize it to have a maximum width of 600 pixels    \n",
    "    frame = imutils.resize(frame, width=600)\n",
    "    # grab the frame dimensions and convert it to a blob\n",
    "    (h, w) = frame.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)),0.007843, (300, 300), 127.5)\n",
    "    # Blob from current frame of traffic sign video\n",
    "    tf_blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416),swapRB=True, crop=False)\n",
    "    # pass the blob through the network and obtain the detections and predictions\n",
    "    net.setInput(blob)\n",
    "    d_net.setInput(tf_blob)\n",
    "    detections = net.forward()\n",
    "    tf_detections = d_net.forward(layers_names_output)\n",
    "       \n",
    "    # loop over the detections\n",
    "    for i in np.arange(0, detections.shape[2]):\n",
    "        # extract the confidence (i.e., probability) associated with the prediction\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        # filter out weak detections by ensuring the `confidence` is greater than the minimum confidence\n",
    "        if confidence > args.confidence:\n",
    "            # extract the index of the class label from the`detections`, then compute the (x, y)coordinates of the bounding box for the object\n",
    "            idx = int(detections[0, 0, i, 1])\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")            \n",
    "            # draw the prediction on the frame\n",
    "            label = \"{}: {:.2f}%\".format(labels[idx],confidence * 100)\n",
    "            cv2.rectangle(frame, (startX, startY), (endX, endY),COLORS[idx], 1)\n",
    "            y = startY - 15 if startY - 15 > 15 else startY + 15\n",
    "            cv2.putText(frame, label, (startY, y),cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLORS[idx], 1)  \n",
    "            print(label)\n",
    "    \n",
    "    pos_dict = dict()\n",
    "    coordinates = dict()\n",
    "    # Focal length (in cm)\n",
    "    F = 50    \n",
    "    box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "    (startX, startY, endX, endY) = box.astype(\"int\")  \n",
    "    coordinates[i] = (startX, startY, endX, endY)\n",
    "    # Mid point of bounding box\n",
    "    x_mid = round((startX+endX)/2,4)\n",
    "    y_mid = round((startY+endY)/2,4)\n",
    "    height = round(endY-startY,4)\n",
    "\n",
    "    # Distance from camera based on triangle similarity\n",
    "    distance = round(((165 * F)/height)/30.48,2)\n",
    "    print(\"Distance:{dist}\".format(dist = distance), \"feet\")\n",
    "    \n",
    "    # Mid-point of bounding boxes (in cm) based on triangle similarity technique\n",
    "    x_mid_cm = (x_mid * distance) / F\n",
    "    y_mid_cm = (y_mid * distance) / F\n",
    "    pos_dict[i] = (x_mid_cm,y_mid_cm,distance)\n",
    "    \n",
    "    # Distance between every object detected in a frame\n",
    "    close_objects = set()\n",
    "    for i in pos_dict.keys():\n",
    "        for j in pos_dict.keys():\n",
    "            if i < j:\n",
    "                dist = sqrt(pow(pos_dict[i][0]-pos_dict[j][0],2) + pow(pos_dict[i][1]-pos_dict[j][1],2) + pow(pos_dict[i][2]-pos_dict[j][2],2))\n",
    "\n",
    "                # Check if distance less than 1 feet (300 mm approx):\n",
    "                if dist < 30:\n",
    "                    close_objects.add(i)\n",
    "                    close_objects.add(j)\n",
    "    for i in pos_dict.keys():\n",
    "        if i in close_objects:\n",
    "            COLOR = (0,0,255)\n",
    "        else:\n",
    "            COLOR = (0,255,0)     \n",
    "        (startX, startY, endX, endY) = coordinates[i]\n",
    "        cv2.rectangle(frame,(startX,startY), (endX, endY), COLOR, 1)\n",
    "        y = startY - 15 if startY - 15 > 15 else startY + 15        \n",
    "        # Convert mms to feet\n",
    "        cv2.putText(frame, \"Distance: {i} ft\".format(i=round(pos_dict[i][2]/30.48,4)), (y, startY),cv2.FONT_HERSHEY_SIMPLEX, 0.45, COLOR, 1)\n",
    "        cv2.namedWindow('Frame',cv2.WINDOW_NORMAL) \n",
    "\n",
    "    # Lists for detected bounding boxes, confidences and class's number\n",
    "    bounding_boxes = []\n",
    "    confidences = []\n",
    "    class_numbers = []\n",
    "\n",
    "    # Going through all output layers after feed forward pass\n",
    "    for traffic_result in tf_detections:\n",
    "        # Going through all detections from current output layer\n",
    "        for detected_objects in traffic_result:\n",
    "            # Getting 80 classes' probabilities for current detected object\n",
    "            scores = detected_objects[8:]\n",
    "            # Getting index of the class with the maximum value of probability\n",
    "            class_current = np.argmax(scores)\n",
    "            # Getting value of probability for defined class\n",
    "            confidence_current = scores[class_current]\n",
    "            # Minimum probability to eliminate weak detections\n",
    "            probability_minimum = 0.9\n",
    "            # Setting threshold to filtering weak bounding boxes by non-maximum suppression\n",
    "            threshold = 0.8\n",
    "            \n",
    "            # Eliminating weak predictions by minimum probability\n",
    "            if confidence_current > probability_minimum:\n",
    "                # Scaling bounding box coordinates to the initial frame size\n",
    "                box_current = detected_objects[0:4] * np.array([w, h, w, h])\n",
    "\n",
    "                # Getting top left corner coordinates\n",
    "                x_center, y_center, box_width, box_height = box_current\n",
    "                x_min = int(x_center - (box_width / 2))\n",
    "                y_min = int(y_center - (box_height / 2))\n",
    "\n",
    "                # Adding results into prepared lists\n",
    "                bounding_boxes.append([x_min, y_min, int(box_width), int(box_height)])\n",
    "                confidences.append(float(confidence_current))\n",
    "                class_numbers.append(class_current)                \n",
    "\n",
    "    # Implementing non-maximum suppression of given bounding boxes\n",
    "    tf_results = cv2.dnn.NMSBoxes(bounding_boxes, confidences, probability_minimum, threshold)\n",
    "\n",
    "    # Checking if there is any detected object been left\n",
    "    if len(tf_results) > 0:\n",
    "        # Going through indexes of results\n",
    "        for i in tf_results.flatten():\n",
    "            # Bounding box coordinates, its width and height\n",
    "            x_min, y_min = bounding_boxes[i][0], bounding_boxes[i][1]\n",
    "            box_width, box_height = bounding_boxes[i][2], bounding_boxes[i][3]             \n",
    "            # Cut fragment with Traffic Sign\n",
    "            c_ts = frame[y_min:y_min+int(box_height), x_min:x_min+int(box_width), :]            \n",
    "            if c_ts.shape[:1] == (0,) or c_ts.shape[1:2] == (0,):\n",
    "                pass\n",
    "            else:\n",
    "                # Getting preprocessed blob with Traffic Sign of needed shape\n",
    "                blob_ts = cv2.dnn.blobFromImage(c_ts, 1 / 255.0, size=(32, 32), swapRB=True, crop=False)\n",
    "                blob_ts[0] = blob_ts[0, :, :, :] - mean['mean_image_rgb']\n",
    "                blob_ts = blob_ts.transpose(0, 2, 3, 1)\n",
    "                               \n",
    "                prediction = np.argmax(scores)\n",
    "                \n",
    "                # Drawing bounding box on the original current frame\n",
    "                cv2.rectangle(frame, (x_min, y_min),(x_min + box_width, y_min + box_height),(0,0,255), 1)\n",
    "\n",
    "                # Preparing text with label and confidence for current bounding box\n",
    "                box = '{}: {:.4f}'.format(tf_labels['SignName'][prediction],confidences[i]*100)\n",
    "\n",
    "                # Putting text with label and confidence on the original image\n",
    "                cv2.putText(frame, box, (x_min, y_min - 5),cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,200), 1)\n",
    "            print(box)\n",
    "\n",
    "    try:\n",
    "        face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "        foundFace = False\n",
    "        user = None\n",
    "        confidence = 82\n",
    "        for key in models:\n",
    "            if foundFace == True:\n",
    "                break\n",
    "            results = models[key][\"model\"].predict(face)\n",
    "            if results[1] < 500:\n",
    "                confidence = int( 100 * (1 - (results[1])/500) )\n",
    "                if confidence > 82:\n",
    "                    user = key\n",
    "                    foundFace = True        \n",
    "        posX = pos[0] + 5\n",
    "        posY = pos[0] - 5\n",
    "        cv2.putText(frame, \"Face Detected \" + str(confidence) + \"%\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 1)\n",
    "        if foundFace == True:\n",
    "            cv2.putText(frame, user, (posX, posY), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 2)   \n",
    "            print(user)\n",
    "            print(\"Face Detected \" + str(confidence) + \"%\")\n",
    "        else:\n",
    "            cv2.putText(frame, \"Unknown \", (posX, posY), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,153,255), 2)\n",
    "\n",
    "        cv2.namedWindow('Frame',cv2.WINDOW_NORMAL)\n",
    "    # Raise exception in case, no image is found\n",
    "    except Exception as e:\n",
    "        cv2.putText(frame, \"Accuracy 0% (No face detected)\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,155,255), 1)\n",
    "        cv2.namedWindow('Frame',cv2.WINDOW_NORMAL)\n",
    "        pass\n",
    "    \n",
    "    # Show the output frame\n",
    "    cv2.imshow('Frame', frame)\n",
    "    cv2.resizeWindow('Frame',800,600)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "    #update the FPS counter\n",
    "    fps.update()  \n",
    "#stop the timer and display FPS count \n",
    "fps.stop()\n",
    "print(\"Elapsed time: {:.2f}\".format(fps.elapsed()))\n",
    "print(\"Approximate FPS: {:.2f}\".format(fps.fps()))\n",
    "\n",
    "import glob\n",
    "for img in glob.glob(\"*.jpg\"):\n",
    "    cv_img = cv2.imread(img)\n",
    "demo = Image.fromarray(cv_img)\n",
    "pytesseract.pytesseract.tesseract_cmd = 'C:/Program Files (x86)/Tesseract-OCR/tesseract.exe'\n",
    "text_reader = pytesseract.image_to_string(demo, lang = 'eng')\n",
    "print(text_reader)    \n",
    "# Clean\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyttsx3\n",
      "  Downloading pyttsx3-2.90-py3-none-any.whl (39 kB)\n",
      "Collecting pypiwin32Note: you may need to restart the kernel to use updated packages.\n",
      "  Downloading pypiwin32-223-py3-none-any.whl (1.7 kB)\n",
      "Requirement already satisfied: pywin32 in c:\\users\\ananya\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pyttsx3) (300)\n",
      "Collecting comtypes\n",
      "  Downloading comtypes-1.1.10.tar.gz (145 kB)\n",
      "Building wheels for collected packages: comtypes\n",
      "  Building wheel for comtypes (setup.py): started\n",
      "  Building wheel for comtypes (setup.py): finished with status 'done'\n",
      "  Created wheel for comtypes: filename=comtypes-1.1.10-py3-none-any.whl size=164919 sha256=e85a9b0c6d615924ab7607c68be86b4ccbd2b7965436666580bc64bad28ad33e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.0.1; however, version 21.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\ananya\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Stored in directory: c:\\users\\ananya\\appdata\\local\\pip\\cache\\wheels\\76\\c4\\bb\\651111ab6c5e31ad6a1ff3022bfdbef5a4e88bfe568150870c\n",
      "Successfully built comtypes\n",
      "Installing collected packages: pypiwin32, comtypes, pyttsx3\n",
      "Successfully installed comtypes-1.1.10 pypiwin32-223 pyttsx3-2.90\n"
     ]
    }
   ],
   "source": [
    "pip install pyttsx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-05b876fe2ea8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'label' is not defined"
     ]
    }
   ],
   "source": [
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_reader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-dddf18da1656>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mrate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetProperty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'rate'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetProperty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'rate'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m160\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mstring\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0muser\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"Face Detected \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfidence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"%\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"Distance:{dist}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"feet\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtext_reader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_to_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"demo.mp3\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text_reader' is not defined"
     ]
    }
   ],
   "source": [
    "import pyttsx3 \n",
    "engine = pyttsx3.init()\n",
    "voices = engine.getProperty('voices')\n",
    "for voice in voices:\n",
    "    engine.setProperty('voice', voices[1].id)\n",
    "rate = engine.getProperty('rate')\n",
    "engine.setProperty('rate', 160)\n",
    "string = label + user + \"Face Detected \",str(confidence),\"%\" + \"Distance:{dist}\".format(dist = distance), \"feet\" + text_reader\n",
    "engine.say(string)\n",
    "engine.save_to_file(string, \"demo.mp3\")\n",
    "engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_reader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-1234070f01d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_reader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbox\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Distance:{dist}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"feet\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text_reader' is not defined"
     ]
    }
   ],
   "source": [
    "%%capture cap --no-stderr\n",
    "print(text_reader)\n",
    "print(label)\n",
    "print(user)\n",
    "print(box)\n",
    "print(\"Distance:{dist}\".format(dist = distance), \"feet\")\n",
    "print(\"Face Detected \", str(confidence),\"%\")\n",
    "with open('output.txt', 'w') as f:\n",
    "    f.write(str(cap)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
