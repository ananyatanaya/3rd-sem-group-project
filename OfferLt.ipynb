{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ananyatanaya/3rd-sem-group-project/blob/master/OfferLt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvpfHrfgr-Fa"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14ouw2imP4e0"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bM8Zji_uQy_K"
      },
      "outputs": [],
      "source": [
        "os.listdir('/content')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvQAxIqrR6en"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/MyDrive/VF_12.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISoubDLHR6YM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOecW7LksARM"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Replace 'your_folder.zip' with the name of your uploaded zip file\n",
        "with zipfile.ZipFile('/content/VF_12.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/your_folder')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "uxbwDYTkxbdh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "folder_path = '/content/your_folder/VF_12'\n",
        "print(os.listdir(folder_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlqDwkxAzvxH"
      },
      "outputs": [],
      "source": [
        "print(\"Files in Folder:\",os.listdir(folder_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "uelqPgLp00tH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Replace 'your_file.xlsx' with an actual file name from the folder\n",
        "file_path = '/content/your_unzipped_folder/VF/content/M-3 Forecast-Group 1-11.xlsx'\n",
        "\n",
        "# Try reading a single .xlsx file\n",
        "data = pd.read_excel(file_path)\n",
        "print(\"Data from the file:\")\n",
        "print(data.head())  # Display the first few rows of the file\n",
        "\n",
        "# Check if it's empty\n",
        "if data.empty:\n",
        "    print(f\"The file {file_path} is empty.\")\n",
        "else:\n",
        "    print(f\"The file {file_path} has data.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Dz4WPv9PWKP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9p8jWZiV1MZP"
      },
      "outputs": [],
      "source": [
        "# If the file contains multiple sheets, check sheet names\n",
        "xlsx = pd.ExcelFile(file_path)\n",
        "print(\"Sheet names:\", xlsx.sheet_names)\n",
        "\n",
        "# Try reading a specific sheet if needed\n",
        "data = pd.read_excel(file_path, sheet_name='Sheet')  # Replace with the correct sheet name if needed\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ZbS-o6Y112aE"
      },
      "outputs": [],
      "source": [
        "# data = pd.read_excel(file_path, header=1)  # Adjust 'header' to the correct row number\n",
        "# print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "biXO8uRe1agv"
      },
      "outputs": [],
      "source": [
        "# # Initialize an empty DataFrame to store the merged data\n",
        "# merged_data = pd.DataFrame()\n",
        "\n",
        "# # Loop through each file and append its content to the merged_data DataFrame\n",
        "# for file in os.listdir(folder_path):\n",
        "#     if file.endswith('.xlsx'):\n",
        "#         file_path = os.path.join(folder_path, file)\n",
        "#         print(f\"Reading file: {file_path}\")\n",
        "\n",
        "#         try:\n",
        "#             data = pd.read_excel(file_path)\n",
        "#             print(f\"Shape of data from {file}: {data.shape}\")\n",
        "#             if not data.empty:\n",
        "#                 merged_data = merged_data.append(data, ignore_index=True)\n",
        "#             else:\n",
        "#                 print(f\"Warning: {file} is empty or has no data.\")\n",
        "#         except Exception as e:\n",
        "#             print(f\"Error reading {file}: {e}\")\n",
        "\n",
        "# # Display the final merged data\n",
        "# print(\"Merged data:\")\n",
        "# print(merged_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RBLrIXZT1nnk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "folder_path = '/content/your_folder/VF_12'  # Replace with your folder path\n",
        "\n",
        "# Get a list of all .xlsx files in the folder\n",
        "xlsx_files = [file for file in os.listdir(folder_path) if file.endswith('.xlsx')]\n",
        "\n",
        "# Check if any .xlsx files are found\n",
        "print(f\"Found {len(xlsx_files)} .xlsx files: {xlsx_files}\")\n",
        "\n",
        "# Initialize an empty list to store DataFrames\n",
        "dataframes = []\n",
        "\n",
        "# Loop through each file and append its content to the list\n",
        "for file in xlsx_files:\n",
        "    file_path = os.path.join(folder_path, file)\n",
        "    print(f\"Reading file: {file_path}\")\n",
        "\n",
        "    try:\n",
        "        data = pd.read_excel(file_path)  # Read each .xlsx file\n",
        "        print(f\"Shape of data from {file}: {data.shape}\")\n",
        "        if not data.empty:\n",
        "            dataframes.append(data)  # Add each DataFrame to the list\n",
        "        else:\n",
        "            print(f\"Warning: {file} is empty or has no data.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {file}: {e}\")\n",
        "\n",
        "# Concatenate all DataFrames into one\n",
        "if dataframes:\n",
        "    merged_data = pd.concat(dataframes, ignore_index=True)\n",
        "    print(\"Merged data:\")\n",
        "    print(merged_data)\n",
        "else:\n",
        "    print(\"No data to merge.\")\n",
        "\n",
        "# Optionally, save the merged data to a new Excel file\n",
        "merged_data.to_excel('merged_data.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raF3j3h9p9GY"
      },
      "outputs": [],
      "source": [
        "merged_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GbzgdzU5G-Ja"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the merged data file\n",
        "merged_data_df = pd.read_excel('merged_data.xlsx')\n",
        "\n",
        "# Check if 'site' column exists\n",
        "if 'Site' in merged_data_df.columns:\n",
        "    # Add a new column 'ERP Plant' with the same values as 'site'\n",
        "    merged_data_df['ERP Plant'] = merged_data_df['Site']\n",
        "else:\n",
        "    print(\"The 'site' column does not exist in the merged data.\")\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(merged_data_df)\n",
        "\n",
        "# Save the updated DataFrame back to the Excel file\n",
        "merged_data_df.to_excel('updated_merged_data.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7HUhLYYpe-t"
      },
      "outputs": [],
      "source": [
        "merged_data_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "FKbz5pv0CFoV"
      },
      "outputs": [],
      "source": [
        "# Concatenate all DataFrames into one\n",
        "if dataframes:\n",
        "    merged_data = pd.concat(dataframes, ignore_index=True)\n",
        "    print(\"Merged data:\")\n",
        "    print(merged_data)\n",
        "else:\n",
        "    print(\"No data to merge.\")\n",
        "\n",
        "# Optionally, save the merged data to a new Excel file\n",
        "merged_data.to_excel('merged_data.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HLUAqUbZ0J0"
      },
      "outputs": [],
      "source": [
        "forecast_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snUciUuBZ6G-"
      },
      "outputs": [],
      "source": [
        "rr_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNLAOw1DaCGx"
      },
      "outputs": [],
      "source": [
        "rr_df = pd.read_excel('/content/ERP Sites 1.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "collapsed": true,
        "id": "pB-2penNLzm1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the forecast accuracy file\n",
        "forecast_df = pd.read_excel('/content/Forecast Accuracy - Vertical.xlsx')\n",
        "\n",
        "# Load the other Excel file containing the RR site value\n",
        "rr_df = pd.read_excel('/content/ERP Sites 1.xlsx)\n",
        "\n",
        "# Assuming the columns are named 'site' in both DataFrames\n",
        "# Merge the DataFrames on the 'site' column\n",
        "merged_df = pd.merge(forecast_df, rr_df[['Site','ERP Plant']], on='Site', how='left')\n",
        "\n",
        "# Extract the ERP plant code\n",
        "# Assuming the ERP plant code is in a column named 'ERP_plant_code' in the forecast_df\n",
        "\n",
        "# Print or return the ERP plant codes\n",
        "# print(merged_df)\n",
        "if 'Unnamed: 14' in merged_df.columns:\n",
        "    merged_df.drop(columns=['Unnamed: 14'],inplace=True)\n",
        "    print(\"Column 'Unnamed:14 has been droped.'\")\n",
        "else:\n",
        "    print(\"The 'Unnamed: 14' column does not exist in the merged data.\")\n",
        "print(merged_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjEGaYayY0Al"
      },
      "outputs": [],
      "source": [
        "merged_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHxM9GDPdAfI"
      },
      "outputs": [],
      "source": [
        "print(merged_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uaMASJRWd3oo"
      },
      "outputs": [],
      "source": [
        "concatenated_df= pd.concat([merged_df,merged_data_df],ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhHDY1rDm-45"
      },
      "outputs": [],
      "source": [
        "concatenated_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-gZtIa1nBzN"
      },
      "outputs": [],
      "source": [
        "concatenated_df.to_excel('Concatenated data.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrFi2-Hzn8yk"
      },
      "outputs": [],
      "source": [
        "!pip install requests beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0abapUOOewYF"
      },
      "outputs": [],
      "source": [
        "!pip install scrapy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjMxfkvMe2N3"
      },
      "outputs": [],
      "source": [
        "pip install selenium\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgcWx9DV4lni"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Load the JSON data\n",
        "with open('data-25thmar.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "    data=data['data']\n",
        "# Convert JSON to DataFrame\n",
        "df = pd.json_normalize(data)\n",
        "\n",
        "# Write DataFrame to Excel\n",
        "df.to_excel('Output-25thmar.xlsx', index=False)\n",
        "# Convert JSON to DataFrame\n",
        "\n",
        "# Write DataFrame to Excel\n",
        "#df.to_excel('output-20thJan.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sic9366JDCWX"
      },
      "outputs": [],
      "source": [
        "df.head(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eI7PInf9BDRx"
      },
      "outputs": [],
      "source": [
        "from os.path import join\n",
        "import pandas as pd\n",
        "df = pd.read_excel(\"/content/MVKE.XLSX\", usecols=[\"Material\",\"Minimum order qty\",\"Minimum delivery qty\",\"Delivery unit\"])\n",
        "print(df)\n",
        "#df.to_excel(\"/content/MVKE.XLSX\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xNR2dbPt46X"
      },
      "outputs": [],
      "source": [
        "from os.path import join\n",
        "import pandas as pd\n",
        "df = pd.read_excel(\"/content/MARC.XLSX\", usecols=[\"Material\",\"Minimum Lot Size\",\"Rounding value\",\"Planned Deliv. Time\"])\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DMuO6ybLzafG"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Upload the ZIP file\n",
        "uploaded = files.upload()  # This will prompt you to upload \"mrp.zip\"\n",
        "\n",
        "# Extract the ZIP file\n",
        "with zipfile.ZipFile(\"MRP.zip\", \"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"MRP\")  # Extracts to \"mrp\" folder\n",
        "\n",
        "# Verify the files\n",
        "os.listdir(\"MRP\")  # List files inside the extracted folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XF_GTBPzv1B"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Define ZIP file name (change if needed)\n",
        "zip_file = \"MRP.zip\"\n",
        "\n",
        "# Extract ZIP file into 'mrp' folder\n",
        "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"mrp\")  # Extracts into 'mrp' directory\n",
        "\n",
        "# Verify extraction\n",
        "print(\"✅ ZIP file extracted successfully!\")\n",
        "print(os.listdir(\"mrp\"))  # List extracted files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhz878orDOXn"
      },
      "outputs": [],
      "source": [
        "merged.head(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkGQkEZp0Xih"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# Define the folder where the files were extracted\n",
        "folder_path = \"/content/MRP/MRP\"  # Folder containing extracted files\n",
        "\n",
        "# Get a list of all Excel files in the folder\n",
        "file_list = glob.glob(os.path.join(folder_path, \"*.xlsx\"))  # Finds all .xlsx files\n",
        "\n",
        "# Read and merge all files\n",
        "df_list = [pd.read_excel(file) for file in file_list]  # Read each file into a DataFrame\n",
        "merged_df = pd.concat(df_list, ignore_index=True)  # Append row-wise\n",
        "\n",
        "# Save the merged DataFrame as an Excel file\n",
        "merged_df.to_excel(\"merged.xlsx\", index=False)\n",
        "\n",
        "print(\"✅ All Excel files merged successfully into 'merged.xlsx'!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QA3gyq5a2ViI"
      },
      "outputs": [],
      "source": [
        "#Offer LT calculation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "#Load the Excel file\n",
        "file_path = \"/content/OneMMToolExport_20250325_071103.csv\"  # Change this to your actual file path\n",
        "df = pd.read_csv(file_path)\n",
        "df.head(4)\n",
        "\n",
        "\n",
        "\n",
        "# Define relevant columns (modify column names as per your Excel file)\n",
        "plant_code_col = \"Plant Code\"\n",
        "commercial_ref_col = \"Commercial Ref.\"\n",
        "stocking_policy_col = \"Stocking Policy\"\n",
        "ss3_qty_col = \"SS3 Qty\"\n",
        "current_stock_col = \"Current Stock Qty\"\n",
        "open_po_col = \"Open PO Qty (Total)\"  # Open purchase order quantity\n",
        "theo_ss_hs_qty_col = \"HS Theo. Prov. Qty\"  # Theoretical SS HS quantity\n",
        "\n",
        "df[ss3_qty_col]= df[ss3_qty_col].replace(0, pd.NA)\n",
        "\n",
        "\n",
        "# Select only required columns\n",
        "columns_needed = [plant_code_col, commercial_ref_col, stocking_policy_col, ss3_qty_col, current_stock_col,open_po_col,theo_ss_hs_qty_col]\n",
        "df_filtered = df[columns_needed]\n",
        "\n",
        "# Remove duplicates based on plant code (if multiple rows exist per plant)\n",
        "#df_final = df_filtered.drop_duplicates(subset=[plant_code_col])\n",
        "\n",
        "# Save the extracted data to a new Excel file\n",
        "output_file = \"filtered_data.xlsx\"\n",
        "# df_final.to_excel(output_file, index=False)\n",
        "\n",
        "# print(f\"Extracted data saved to {output_file}\")\n",
        "\n",
        "# file_path = \"filtered_data.xlsx\"  # Ensure this file exists from the previous step\n",
        "# df = pd.read_excel(file_path)\n",
        "\n",
        "\n",
        "\n",
        "# Define additional required column names (modify if needed)\n",
        "\n",
        "\n",
        "# Ensure the required columns exist in the dataset before calculations\n",
        "required_columns = [ss3_qty_col, current_stock_col, open_po_col, theo_ss_hs_qty_col]\n",
        "for col in required_columns:\n",
        "    if col not in df.columns:\n",
        "        df[col] = 0  # Add missing columns with default value 0 if not\n",
        "\n",
        "# Perform calculations\n",
        "df[\"SS3 Cover with SOH\"] = df[\"Current Stock Qty\"] / df[\"SS3 Qty\"]\n",
        "df[\"SS3 Coverage with SOH + PO\"] = (df[\"Current Stock Qty\"] + df[\"Open PO Qty (Total)\"]) / df[\"SS3 Qty\"]\n",
        "df[\"SS3 Coverage with Excess\"] = df[\"HS Theo. Prov. Qty\"] / df[\"SS3 Qty\"]\n",
        "\n",
        "# Save the calculated data to a new Excel file\n",
        "output_file = \"calculated_data.xlsx\"\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "df[theo_ss_hs_qty_col] = pd.to_numeric(df[theo_ss_hs_qty_col], errors=\"coerce\")\n",
        "df[ss3_qty_col] = pd.to_numeric(df[ss3_qty_col], errors=\"coerce\")\n",
        "\n",
        "# Filter only SG20 plant and HS Theo. Prov. Qty ≤ SS3 Qty\n",
        "df_filtered = df[(df[plant_code_col] == \"SG20\") & (df[theo_ss_hs_qty_col] <= df[ss3_qty_col])]\n",
        "print(f\"Calculated data saved to {output_file}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "new_file_path = \"/content/POPiloting__2025-03-20_10-55.xlsx\"\n",
        "df_new = pd.read_excel(new_file_path)\n",
        "df_new.head(3)\n",
        "open_qty_col = \"Open Quantity\"\n",
        "last_conf_date_col = \"Last Confirmation Date (AB/LA/1st)\"\n",
        "# Filter only for Plant SG20 where \"HS Theo. Prov. Qty\" <= \"SS3 Qty\"\n",
        "# Sum the Open Quantity for SG20\n",
        "#total_open_qty_sg20 = df_sg20[open_qty_col].sum()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q0ZxUF9_5zkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# File paths\n",
        "calculated_file = \"/content/calculated_data.xlsx\"  # File with SS3 Quantity\n",
        "new_excel_file = \"/content/POPiloting__2025-03-20_10-55.xlsx\"  # New Excel file with Open Quantity and Last Confirmation Date\n",
        "\n",
        "# Define column names (modify as per actual file)\n",
        "plant_code_col = \"Plant Code\"\n",
        "plant_code_colE2E = \"Plant code\"\n",
        "open_qty_col = \"Open Quantity\"\n",
        "ss3_qty_col = \"SS3 Qty\"\n",
        "last_confirm_date_col = \"Last confirmation date (AB/LA/1st)\"\n",
        "\n",
        "# Load the new Excel file\n",
        "df_new = pd.read_excel(new_excel_file)\n",
        "print(list(df_new.columns))\n",
        "\n",
        "# Load the calculated data file\n",
        "df_calculated = pd.read_excel(calculated_file)\n",
        "print(list(df_calculated.columns))\n",
        "\n",
        "# Ensure required columns exist\n",
        "required_columns_new = [plant_code_colE2E, open_qty_col, last_confirm_date_col]\n",
        "required_columns_calc = [plant_code_col, ss3_qty_col]\n",
        "\n",
        "for col in required_columns_new:\n",
        "    if col not in df_new.columns:\n",
        "        raise ValueError(f\"Missing column in new Excel: {col}\")\n",
        "\n",
        "for col in required_columns_calc:\n",
        "    if col not in df_calculated.columns:\n",
        "        raise ValueError(f\"Missing column in calculated data: {col}\")\n",
        "\n",
        "# Convert numeric columns to avoid issues with calculations\n",
        "df_new[open_qty_col] = pd.to_numeric(df_new[open_qty_col], errors=\"coerce\")\n",
        "df_calculated[ss3_qty_col] = pd.to_numeric(df_calculated[ss3_qty_col], errors=\"coerce\")\n",
        "\n",
        "# Step 1: Filter for SG20 plant in both datasets\n",
        "df_new_sg20 = df_new[df_new[plant_code_colE2E] == \"SG20\"]\n",
        "df_calculated_sg20 = df_calculated[df_calculated[plant_code_col] == \"SG20\"]\n",
        "\n",
        "# Step 2: Sum \"Open Quantity\" for SG20\n",
        "total_open_qty = df_new_sg20[open_qty_col].sum()\n",
        "\n",
        "# Step 3: Get SS3 Quantity for SG20\n",
        "ss3_qty_sg20 = df_calculated_sg20[ss3_qty_col].sum()\n",
        "\n",
        "# Initialize new column with NaN\n",
        "df_calculated['PO date to RPL'] = None\n",
        "df_calculated['rplt_lt2_col'] = None\n",
        "\n",
        "\n",
        "# Check condition: total_open_qty <= ss3_qty_sg20\n",
        "if total_open_qty <= ss3_qty_sg20:\n",
        "    # Convert \"Last Confirmation Date\" to datetime\n",
        "    df_new_sg20[last_confirm_date_col] = pd.to_datetime(df_new_sg20[last_confirm_date_col], errors=\"coerce\")\n",
        "\n",
        "    # Get the latest date for SG20\n",
        "    latest_confirmation_date = df_new_sg20[last_confirm_date_col].max()\n",
        "\n",
        "    # Add this date to the SG20 row in calculated data\n",
        "    df_calculated.loc[df_calculated[plant_code_col] == \"SG20\", 'PO date to RPL'] = latest_confirmation_date\n",
        "\n",
        "# Save the updated calculated data to a new Excel file\n",
        "df_calculated.to_excel(output_file, index=False)\n",
        "\n",
        "# Calculate RPLT LT2 for SG20\n",
        "today_date = datetime.today()\n",
        "\n",
        "df_new_sg20['rplt_lt2_col'] = (df_new_sg20['PO date to RPL'] - today_date).dt.days / 7  # Convert to weeks\n",
        "df_new_sg20['rplt_lt2_col'] = df_new_sg20['rplt_lt2_col'].astype(\"Int64\")  # Format as whole numbers\n",
        "\n",
        "# Merge RPLT LT2 values back to calculated data for SG20\n",
        "df_calculated = df_calculated.merge(\n",
        "    df_new_sg20[[plant_code_col, rplt_lt2_col]],\n",
        "    on=plant_code_col,\n",
        "    how=\"left\"\n",
        ")\n",
        "# Offer LT= RPLT LT 1 + 1 #[For all plants]\n",
        "# # # # Save the updated calculated data to a new Excel file\n",
        "# # # df_calculated.to_excel(output_file, index=False)\n",
        "print(f\"Updated data saved to {output_file}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ZZ8eMhzKCPLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#Load the Excel file\n",
        "file_path = \"/content/OneMMToolExport_20250320_110327.csv\"  # Change this to your actual file path\n",
        "df = pd.read_csv(file_path)\n",
        "df.head(4)\n",
        "\n",
        "\n",
        "\n",
        "# Define relevant columns (modify column names as per your Excel file)\n",
        "plant_code_col = \"Plant Code\"\n",
        "commercial_ref_col = \"Commercial Ref.\"\n",
        "stocking_policy_col = \"Stocking Policy\"\n",
        "ss3_qty_col = \"SS3 Qty\"\n",
        "current_stock_col = \"Current Stock Qty\"\n",
        "open_po_col = \"Open PO Qty (Total)\"  # Open purchase order quantity\n",
        "theo_ss_hs_qty_col = \"HS Theo. Prov. Qty\"  # Theoretical SS HS quantity\n",
        "\n",
        "# Select only required columns\n",
        "columns_needed = [plant_code_col, commercial_ref_col, stocking_policy_col, ss3_qty_col, current_stock_col,open_po_col,theo_ss_hs_qty_col]\n",
        "df_filtered = df[columns_needed…\n",
        "[10:54, 3/25/2025] Ananya: import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# File paths\n",
        "calculated_file = \"calculated_data.xlsx\"\n",
        "new_excel_file = \"new_data.xlsx\"\n",
        "output_file = \"updated_calculated_data.xlsx\"\n",
        "\n",
        "# Define column names (modify if needed)\n",
        "plant_code_col = \"Plant Code\"\n",
        "open_qty_col = \"Open Quantity\"\n",
        "ss3_qty_col = \"SS3 Quantity\"\n",
        "last_confirm_date_col = \"Last Confirmation Date\"\n",
        "po_date_rpl_col = \"PO Date to RPL\"\n",
        "commercial_ref_col = \"Commercial Ref\"  # Added Commercial Reference Column\n",
        "new_date_col = \"Latest Confirmation Date\"\n",
        "rplt_lt2_col = \"RPLT LT2\"\n",
        "\n",
        "# Load data from both Excel files\n",
        "df_new = pd.read_excel(new_excel_file)\n",
        "df_calculated = pd.read_excel(calculated_file)\n",
        "\n",
        "# Ensure required columns exist\n",
        "required_columns_new = [plant_code_col, open_qty_col, last_confirm_date_col, po_date_rpl_col, commercial_ref_col]\n",
        "required_columns_calc = [plant_code_col, ss3_qty_col, commercial_ref_col]\n",
        "\n",
        "for col in required_columns_new:\n",
        "    if col not in df_new.columns:\n",
        "        raise ValueError(f\"Missing column in new Excel: {col}\")\n",
        "\n",
        "for col in required_columns_calc:\n",
        "    if col not in df_calculated.columns:\n",
        "        raise ValueError(f\"Missing column in calculated data: {col}\")\n",
        "\n",
        "# Convert numeric columns to avoid issues\n",
        "df_new[open_qty_col] = pd.to_numeric(df_new[open_qty_col], errors=\"coerce\")\n",
        "df_calculated[ss3_qty_col] = pd.to_numeric(df_calculated[ss3_qty_col], errors=\"coerce\")\n",
        "\n",
        "# Convert date columns\n",
        "df_new[last_confirm_date_col] = pd.to_datetime(df_new[last_confirm_date_col], errors=\"coerce\")\n",
        "df_new[po_date_rpl_col] = pd.to_datetime(df_new[po_date_rpl_col], errors=\"coerce\")\n",
        "\n",
        "# Filter only SG20 data\n",
        "df_new_sg20 = df_new[df_new[plant_code_col] == \"SG20\"]\n",
        "df_calculated_sg20 = df_calculated[df_calculated[plant_code_col] == \"SG20\"]\n",
        "\n",
        "# Sum \"Open Quantity\" for SG20\n",
        "total_open_qty = df_new_sg20[open_qty_col].sum()\n",
        "\n",
        "# Get SS3 Quantity for SG20\n",
        "ss3_qty_sg20 = df_calculated_sg20[ss3_qty_col].sum()\n",
        "\n",
        "# Initialize new columns in the calculated data\n",
        "df_calculated[new_date_col] = None\n",
        "df_calculated[rplt_lt2_col] = None\n",
        "\n",
        "# Check condition: total_open_qty <= ss3_qty_sg20\n",
        "if total_open_qty <= ss3_qty_sg20:\n",
        "    latest_confirmation_date = df_new_sg20[last_confirm_date_col].max()\n",
        "    df_calculated.loc[df_calculated[plant_code_col] == \"SG20\", new_date_col] = latest_confirmation_date\n",
        "\n",
        "# Calculate RPLT LT2 for SG20\n",
        "today_date = datetime.today()\n",
        "df_new_sg20[rplt_lt2_col] = (df_new_sg20[po_date_rpl_col] - today_date).dt.days / 7\n",
        "df_new_sg20[rplt_lt2_col] = df_new_sg20[rplt_lt2_col].astype(\"Int64\")\n",
        "\n",
        "# Merge using both \"Plant Code\" and \"Commercial Ref\" to keep all rows\n",
        "df_calculated = df_calculated.merge(\n",
        "    df_new_sg20[[plant_code_col, commercial_ref_col, rplt_lt2_col]],\n",
        "    on=[plant_code_col, commercial_ref_col],\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "# Save the updated calculated data to a new Excel file\n",
        "df_calculated.to_excel(output_file, index=False)\n",
        "\n",
        "print(f\"Updated data saved to {output_file}\")\n"
      ],
      "metadata": {
        "id": "0ddjTxA49cl8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQvLuAMoUs9+bbIitZQgha",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}